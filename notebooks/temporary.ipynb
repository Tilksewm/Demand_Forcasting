{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410900da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "#creating the model\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import os\n",
    "import joblib\n",
    "from config import DATA_PROCESSED as DATA_PATH\n",
    "from config import MODELS_DIR as MODEL_PATH\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "FEATURES_BASE = [\n",
    "    \"day_of_week\",\n",
    "    \"is_weekend\",\n",
    "    \"day_of_year\",\n",
    "    \"qty_lag_1\",\n",
    "    \"qty_lag_7\",\n",
    "    \"qty_lag_14\",\n",
    "    \"qty_roll_7\",\n",
    "    \"qty_roll_14\",\n",
    "    \"is_holiday\",\n",
    "    \"days_to_holiday\",\n",
    "    \"days_since_holiday\",\n",
    "    \"is_fasting\",\n",
    "]\n",
    "\n",
    "TARGET = \"target_qty\"\n",
    "MIN_HISTORY_DAYS = 90\n",
    "\n",
    "# DATA_PATH = \"/content/drive/MyDrive/ML Projects/Sales_Forecasting_Project/data/processed/\"\n",
    "# MODEL_PATH = \"/content/drive/MyDrive/ML Projects/Sales_Forecasting_Project/models/\"\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# LOAD DATA\n",
    "# ----------------------------\n",
    "train_df = pd.read_csv(DATA_PATH + \"train_data.csv\", parse_dates=[\"date\"])\n",
    "test_df  = pd.read_csv(DATA_PATH + \"test_data.csv\",  parse_dates=[\"date\"])\n",
    "\n",
    "train_df = train_df.sort_values([\"sku_id\", \"date\"])\n",
    "test_df  = test_df.sort_values([\"sku_id\", \"date\"])\n",
    "\n",
    "# ----------------------------\n",
    "# FILTER SKUs WITH TOO LITTLE DATA\n",
    "# ----------------------------\n",
    "valid_skus = (\n",
    "    train_df.groupby(\"sku_id\")\n",
    "    .size()\n",
    "    .loc[lambda x: x >= MIN_HISTORY_DAYS]\n",
    "    .index\n",
    ")\n",
    "\n",
    "train_df = train_df[train_df[\"sku_id\"].isin(valid_skus)]\n",
    "test_df  = test_df[test_df[\"sku_id\"].isin(valid_skus)]\n",
    "\n",
    "# ----------------------------\n",
    "# CATEGORY TARGET ENCODING (TRAIN ONLY)\n",
    "# ----------------------------\n",
    "category_mean = (\n",
    "    train_df\n",
    "    .groupby(\"category\")[TARGET]\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "train_df[\"category_te\"] = train_df[\"category\"].map(category_mean)\n",
    "test_df[\"category_te\"]  = test_df[\"category\"].map(category_mean)\n",
    "\n",
    "global_mean = train_df[TARGET].mean()\n",
    "test_df[\"category_te\"] = test_df[\"category_te\"].fillna(global_mean)\n",
    "\n",
    "# ----------------------------\n",
    "# SKU BEHAVIOR FEATURES (ROLLING)\n",
    "# ----------------------------\n",
    "def add_sku_behavior(df):\n",
    "    df = df.copy()\n",
    "    df[\"sku_avg_28d\"] = (\n",
    "        df.groupby(\"sku_id\")[\"daily_qty\"]\n",
    "        .rolling(28)\n",
    "        .mean()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    df[\"sku_std_28d\"] = (\n",
    "        df.groupby(\"sku_id\")[\"daily_qty\"]\n",
    "        .rolling(28)\n",
    "        .std()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "train_df = add_sku_behavior(train_df)\n",
    "test_df  = add_sku_behavior(test_df)\n",
    "\n",
    "# ----------------------------\n",
    "# FINAL FEATURE LIST\n",
    "# ----------------------------\n",
    "FEATURES = FEATURES_BASE + [\n",
    "    \"category_te\",\n",
    "    \"sku_avg_28d\",\n",
    "    \"sku_std_28d\",\n",
    "]\n",
    "\n",
    "# ----------------------------\n",
    "# DROP NA (ONLY AFTER FEATURES READY)\n",
    "# ----------------------------\n",
    "train_df = train_df.dropna(subset=FEATURES + [TARGET])\n",
    "test_df  = test_df.dropna(subset=FEATURES + [TARGET])\n",
    "\n",
    "# ----------------------------\n",
    "# TRAIN GLOBAL MODEL\n",
    "# ----------------------------\n",
    "model = HistGradientBoostingRegressor(\n",
    "    max_depth=8,\n",
    "    learning_rate=0.05,\n",
    "    max_iter=300,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(train_df[FEATURES], train_df[TARGET])\n",
    "\n",
    "joblib.dump(model, MODEL_PATH + \"global_demand_model.joblib\")\n",
    "\n",
    "# ----------------------------\n",
    "# EVALUATION\n",
    "# ----------------------------\n",
    "test_df[\"pred\"] = model.predict(test_df[FEATURES]).clip(0)\n",
    "\n",
    "overall_mae = mean_absolute_error(test_df[TARGET], test_df[\"pred\"])\n",
    "\n",
    "sku_mae = (\n",
    "    test_df\n",
    "    .groupby(\"sku_id\")\n",
    "    .apply(lambda x: mean_absolute_error(x[TARGET], x[\"pred\"]))\n",
    "    .reset_index(name=\"sku_mae\")\n",
    ")\n",
    "\n",
    "print(f\"Global MAE: {overall_mae:.2f}\")\n",
    "print(\"Sample SKU MAE:\")\n",
    "print(sku_mae.head())\n",
    "\n",
    "sku_mae.to_csv(DATA_PATH + \"global_model_sku_mae.csv\", index=False)\n",
    "\n",
    "print(\"Global model training & evaluation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10b851b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "# Loading data for evaluation\n",
    "# !pip install pandas\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "MODEL_PATH = \"../models/global_demand_model.joblib\"\n",
    "DATA_PATH  = \"../data/processed/\"\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG (Copied from Mv_jgUlwXkQp to ensure consistency)\n",
    "# ----------------------------\n",
    "FEATURES_BASE = [\n",
    "    \"day_of_week\",\n",
    "    \"is_weekend\",\n",
    "    \"day_of_year\",\n",
    "    \"qty_lag_1\",\n",
    "    \"qty_lag_7\",\n",
    "    \"qty_lag_14\",\n",
    "    \"qty_roll_7\",\n",
    "    \"qty_roll_14\",\n",
    "    \"is_holiday\",\n",
    "    \"days_to_holiday\",\n",
    "    \"days_since_holiday\",\n",
    "    \"is_fasting\",\n",
    "]\n",
    "\n",
    "TARGET = \"target_qty\"\n",
    "MIN_HISTORY_DAYS = 90\n",
    "\n",
    "# ----------------------------\n",
    "# LOAD DATA\n",
    "# ----------------------------\n",
    "# Load both train and test data, as train_df is needed for category_te\n",
    "train_df = pd.read_csv(DATA_PATH + \"train_data.csv\", parse_dates=[\"date\"])\n",
    "test_df  = pd.read_csv(DATA_PATH + \"test_data.csv\",  parse_dates=[\"date\"])\n",
    "\n",
    "train_df = train_df.sort_values([\"sku_id\", \"date\"])\n",
    "test_df  = test_df.sort_values([\"sku_id\", \"date\"])\n",
    "\n",
    "# ----------------------------\n",
    "# FILTER SKUs WITH TOO LITTLE DATA (Consistent with training)\n",
    "# ----------------------------\n",
    "valid_skus = (\n",
    "    train_df.groupby(\"sku_id\")\n",
    "    .size()\n",
    "    .loc[lambda x: x >= MIN_HISTORY_DAYS]\n",
    "    .index\n",
    ")\n",
    "\n",
    "train_df = train_df[train_df[\"sku_id\"].isin(valid_skus)]\n",
    "test_df  = test_df[test_df[\"sku_id\"].isin(valid_skus)]\n",
    "\n",
    "# ----------------------------\n",
    "# CATEGORY TARGET ENCODING (Consistent with training)\n",
    "# ----------------------------\n",
    "category_mean = (\n",
    "    train_df\n",
    "    .groupby(\"category\")[TARGET]\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "test_df[\"category_te\"]  = test_df[\"category\"].map(category_mean)\n",
    "\n",
    "global_mean = train_df[TARGET].mean() # Use global mean from training data\n",
    "test_df[\"category_te\"] = test_df[\"category_te\"].fillna(global_mean)\n",
    "\n",
    "# ----------------------------\n",
    "# SKU BEHAVIOR FEATURES (ROLLING) (Consistent with training)\n",
    "# ----------------------------\n",
    "def add_sku_behavior(df):\n",
    "    df = df.copy()\n",
    "    df[\"sku_avg_28d\"] = (\n",
    "        df.groupby(\"sku_id\")[\"daily_qty\"]\n",
    "        .rolling(28)\n",
    "        .mean()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    df[\"sku_std_28d\"] = (\n",
    "        df.groupby(\"sku_id\")[\"daily_qty\"]\n",
    "        .rolling(28)\n",
    "        .std()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "test_df  = add_sku_behavior(test_df)\n",
    "\n",
    "# ----------------------------\n",
    "# FINAL FEATURE LIST (Must match training features)\n",
    "# ----------------------------\n",
    "FEATURES = FEATURES_BASE + [\n",
    "    \"sku_avg_28d\",\n",
    "    \"sku_std_28d\",\n",
    "    \"category_te\",\n",
    "]\n",
    "\n",
    "# ----------------------------\n",
    "# DROP NA (ONLY AFTER FEATURES READY) (Consistent with training)\n",
    "# ----------------------------\n",
    "test_df  = test_df.dropna(subset=FEATURES + [TARGET])\n",
    "\n",
    "model = joblib.load(MODEL_PATH)\n",
    "\n",
    "test_df[\"daily_pred\"] = model.predict(test_df[FEATURES]).clip(0)\n",
    "test_df[\"year_month\"] = test_df[\"date\"].dt.to_period(\"M\")\n",
    "\n",
    "monthly_preds = (\n",
    "    test_df\n",
    "    .groupby([\"sku_id\", \"year_month\"])[\"daily_pred\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"daily_pred\": \"predicted_monthly_qty\"})\n",
    ")\n",
    "\n",
    "# Comparing with actual pridiction\n",
    "actual_monthly = (\n",
    "    test_df\n",
    "    .groupby([\"sku_id\", \"year_month\"])[\"target_qty\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"target_qty\": \"actual_monthly_qty\"})\n",
    ")\n",
    "\n",
    "monthly_results = monthly_preds.merge(\n",
    "    actual_monthly,\n",
    "    on=[\"sku_id\", \"year_month\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "monthly_results[\"abs_error\"] = (\n",
    "    monthly_results[\"predicted_monthly_qty\"]\n",
    "    - monthly_results[\"actual_monthly_qty\"]\n",
    ").abs()\n",
    "\n",
    "monthly_results[\"pct_error\"] = np.where(\n",
    "    monthly_results[\"actual_monthly_qty\"] > 0,\n",
    "    monthly_results[\"abs_error\"] / monthly_results[\"actual_monthly_qty\"] * 100,\n",
    "    0\n",
    ")\n",
    "monthly_results.to_csv(\n",
    "    DATA_PATH + \"global_monthly_predictions_per_sku.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"Monthly per-SKU predictions generated.\")\n",
    "print(monthly_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8891c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize evaluation\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Select a sample SKU, e.g., the first one in the results\n",
    "sample_sku = monthly_results['sku_id'].iloc[0]\n",
    "sample_sku_results = monthly_results[monthly_results['sku_id'] == sample_sku].sort_values('year_month')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x=sample_sku_results['year_month'].astype(str), y='actual_monthly_qty', data=sample_sku_results, label='Actual Quantity', marker='o')\n",
    "sns.lineplot(x=sample_sku_results['year_month'].astype(str), y='predicted_monthly_qty', data=sample_sku_results, label='Predicted Quantity', marker='x')\n",
    "\n",
    "plt.title(f'Actual vs. Predicted Monthly Quantities for SKU: {sample_sku}')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Quantity')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the results generated from code cell 2 (or ensure monthly_results is in scope)\n",
    "# For robustness, we'll load it from the saved CSV.\n",
    "# If you ran code cell 2, `monthly_results` should already be available.\n",
    "try:\n",
    "    monthly_results = pd.read_csv(\"/content/drive/MyDrive/ML Projects/Sales_Forecasting_Project/data/processed/global_monthly_predictions_per_sku.csv\")\n",
    "    monthly_results['year_month'] = pd.PeriodIndex(monthly_results['year_month'], freq='M')\n",
    "except NameError:\n",
    "    print(\"monthly_results DataFrame not found. Please run code cell 2 first.\")\n",
    "    exit()\n",
    "\n",
    "# Calculate Global Metrics for context on the scatter plot\n",
    "overall_mae = monthly_results['abs_error'].mean()\n",
    "overall_mape = monthly_results['pct_error'].mean()\n",
    "\n",
    "print(f\"Overall Mean Absolute Error: {overall_mae:.2f}\")\n",
    "print(f\"Overall Mean Absolute Percentage Error: {overall_mape:.2f}%\")\n",
    "\n",
    "# Plotting Setup\n",
    "plt.style.use('seaborn-v0_8-muted')\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# --- PLOT 1: Scatter Plot (Actual vs Predicted) ---\n",
    "sns.scatterplot(data=monthly_results, x='actual_monthly_qty', y='predicted_monthly_qty', alpha=0.5, ax=ax1)\n",
    "# Add a 45-degree line (Perfect prediction line)\n",
    "max_val = max(monthly_results['actual_monthly_qty'].max(), monthly_results['predicted_monthly_qty'].max())\n",
    "ax1.plot([0, max_val], [0, max_val], color='red', linestyle='--', label='Perfect Prediction')\n",
    "\n",
    "ax1.set_title(f\"Actual vs. Predicted Monthly Sales\\n(Overall MAPE: {overall_mape:.2f}%)\")\n",
    "ax1.set_xlabel(\"Actual Quantity\")\n",
    "ax1.set_ylabel(\"Predicted Quantity\")\n",
    "ax1.legend()\n",
    "\n",
    "# --- PLOT 2: Top 5 SKUs Comparison ---\n",
    "# Get the top 5 SKUs by total actual volume to keep the chart readable\n",
    "top_skus = monthly_results.groupby('sku_id')['actual_monthly_qty'].sum().nlargest(5).index\n",
    "top_df = monthly_results[monthly_results['sku_id'].isin(top_skus)]\n",
    "\n",
    "# Melt the data for easier plotting with Seaborn\n",
    "melted_df = top_df.melt(id_vars=['sku_id', 'year_month'],\n",
    "                        value_vars=['actual_monthly_qty', 'predicted_monthly_qty'],\n",
    "                        var_name='Type', value_name='Quantity')\n",
    "\n",
    "sns.barplot(data=melted_df, x='sku_id', y='Quantity', hue='Type', ax=ax2)\n",
    "ax2.set_title(\"Performance on Top 5 High-Volume SKUs\")\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, ha='right') # Rotate and align labels\n",
    "ax2.legend(title='Quantity Type')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"forecasting_performance.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set visual style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "\n",
    "# Load the results and feature importance (assuming they are saved)\n",
    "results_df = pd.read_csv(\"/content/drive/MyDrive/ML Projects/Sales_Forecasting_Project/data/processed/global_monthly_predictions_per_sku.csv\")\n",
    "results_df['year_month'] = pd.PeriodIndex(results_df['year_month'], freq='M')\n",
    "\n",
    "# NOTE: importance_df is assumed to be available from a previously run cell (like iiQ6cKmvEp3s)\n",
    "# If running this cell independently after a kernel restart, you might need to re-generate it.\n",
    "# For this demonstration, we'll use the existing 'importance_df' variable from the kernel state.\n",
    "\n",
    "# 1. Feature Importance (Average across all SKU models)\n",
    "plt.subplot(3, 2, 1)\n",
    "sns.barplot(data=importance_df, x='Importance', y='Feature', palette='viridis')\n",
    "plt.title(\"1. Which Factors Matter Most? (Global Feature Importance)\")\n",
    "\n",
    "# 2. Average Percentage Error per SKU\n",
    "# (Showing top 20 most 'problematic' SKUs to keep it readable)\n",
    "plt.subplot(3, 2, 2)\n",
    "sku_error = results_df.groupby('sku_id')['pct_error'].mean().sort_values(ascending=False).head(20)\n",
    "sku_error.plot(kind='bar', color='salmon')\n",
    "plt.title(\"2. Top 20 SKUs with Highest Avg % Error\")\n",
    "plt.ylabel(\"Avg % Error\")\n",
    "\n",
    "# 3. Error Distribution\n",
    "plt.subplot(3, 2, 3)\n",
    "sns.histplot(results_df['pct_error'], kde=True, color='skyblue', bins=30)\n",
    "plt.axvline(results_df['pct_error'].median(), color='red', linestyle='--', label='Median Error')\n",
    "plt.title(\"3. Distribution of Percentage Errors\")\n",
    "plt.xlabel(\"% Error\")\n",
    "plt.legend()\n",
    "\n",
    "# 4. Box Plot - Error Spread per SKU\n",
    "# We filter to the top 10 volume SKUs so the boxplot isn't crowded\n",
    "plt.subplot(3, 2, 4)\n",
    "top_10_skus = results_df.groupby('sku_id')['actual_monthly_qty'].sum().nlargest(10).index\n",
    "sns.boxplot(data=results_df[results_df['sku_id'].isin(top_10_skus)], x='sku_id', y='pct_error')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"4. Error Spread (Variance) for Top 10 High-Volume SKUs\")\n",
    "\n",
    "# 5. Deep Dive: One SKU Demand Over Time\n",
    "# We'll pick the SKU with the highest volume for the deep dive\n",
    "plt.subplot(3, 1, 3) # Bottom row, full width\n",
    "target_sku = results_df.groupby('sku_id')['actual_monthly_qty'].sum().idxmax()\n",
    "deep_dive_df = results_df[results_df['sku_id'] == target_sku].sort_values('year_month')\n",
    "\n",
    "plt.plot(deep_dive_df['year_month'].astype(str), deep_dive_df['actual_monthly_qty'], marker='o', label='Actual', linewidth=2)\n",
    "plt.plot(deep_dive_df['year_month'].astype(str), deep_dive_df['predicted_monthly_qty'], marker='s', linestyle='--', label='Predicted', linewidth=2)\n",
    "plt.fill_between(deep_dive_df['year_month'].astype(str), deep_dive_df['actual_monthly_qty'], deep_dive_df['predicted_monthly_qty'], color='gray', alpha=0.2)\n",
    "plt.title(f\"5. Deep Dive: Monthly Demand vs Forecast for SKU: {target_sku}\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Quantity\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"advanced_forecasting_analysis.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5653491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure results_df is loaded (from previous steps)\n",
    "if 'results_df' not in locals():\n",
    "    results_df = pd.read_csv(\"/content/drive/MyDrive/ML Projects/Sales_Forecasting_Project/data/processed/global_monthly_predictions_per_sku.csv\")\n",
    "    results_df['year_month'] = pd.PeriodIndex(results_df['year_month'], freq='M')\n",
    "\n",
    "# Also need test_df to get category information for each sku\n",
    "if 'test_df' not in locals():\n",
    "    DATA_PATH = \"/content/drive/MyDrive/ML Projects/Sales_Forecasting_Project/data/processed/\"\n",
    "    test_df = pd.read_csv(DATA_PATH + \"test_data.csv\",  parse_dates=[\"date\"])\n",
    "    test_df = test_df.sort_values([\"sku_id\", \"date\"])\n",
    "\n",
    "\n",
    "plt.style.use('seaborn-v0_8-muted')\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 12))\n",
    "\n",
    "# --- 1. Error Over Time (Aggregated) ---\n",
    "monthly_avg_error = results_df.groupby('year_month')['pct_error'].mean().reset_index()\n",
    "sns.lineplot(ax=axes[0], x=monthly_avg_error['year_month'].astype(str), y='pct_error', data=monthly_avg_error, marker='o')\n",
    "axes[0].set_title('Average Percentage Error Over Time (All SKUs)')\n",
    "axes[0].set_xlabel('Month')\n",
    "axes[0].set_ylabel('Average % Error')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True)\n",
    "\n",
    "# --- 2. Error by Product Category ---\n",
    "# Merge results with test_df to get category information for each SKU\n",
    "sku_to_category = test_df[['sku_id', 'category']].drop_duplicates()\n",
    "error_by_category = results_df.groupby('sku_id')['pct_error'].mean().reset_index()\n",
    "error_by_category = error_by_category.merge(sku_to_category, on='sku_id', how='left')\n",
    "\n",
    "category_avg_error = error_by_category.groupby('category')['pct_error'].mean().sort_values(ascending=False).reset_index()\n",
    "sns.barplot(ax=axes[1], x='category', y='pct_error', data=category_avg_error, palette='coolwarm')\n",
    "axes[1].set_title('Average Percentage Error by Product Category')\n",
    "axes[1].set_xlabel('Product Category')\n",
    "axes[1].set_ylabel('Average % Error')\n",
    "axes[1].tick_params(axis='x', rotation=90)\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"additional_error_analysis.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175fe24c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a477732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83474abb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1674ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rolling forward test\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "# ----------------------------\n",
    "# Re-load train_df for consistency with global model feature engineering\n",
    "# ----------------------------\n",
    "train_df_for_features = pd.read_csv(\n",
    "    \"/content/drive/MyDrive/ML Projects/Sales_Forecasting_Project/data/processed/train_data.csv\",\n",
    "    parse_dates=[\"date\"]\n",
    ")\n",
    "train_df_for_features = train_df_for_features.sort_values([\"sku_id\", \"date\"])\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "TARGET = \"target_qty\"\n",
    "MIN_HISTORY_DAYS = 90 # Not directly used for filtering in this cell, but good to keep consistent\n",
    "\n",
    "# ----------------------------\n",
    "# Model and Data Paths\n",
    "# ----------------------------\n",
    "MODEL_PATH = \"/content/drive/MyDrive/ML Projects/Sales_Forecasting_Project/models/\"\n",
    "DATA_PATH = \"/content/drive/MyDrive/ML Projects/Sales_Forecasting_Project/data/processed/\"\n",
    "\n",
    "model = joblib.load(\n",
    "    MODEL_PATH + \"global_demand_model.joblib\" # Ensure correct global model name\n",
    ")\n",
    "\n",
    "# Re-load daily to ensure clean slate for feature engineering\n",
    "daily = pd.read_csv(\n",
    "    DATA_PATH + \"train_data.csv\", # Assuming daily is the training data to forecast from\n",
    "    parse_dates=[\"date\"]\n",
    ")\n",
    "daily = daily.sort_values([\"sku_id\", \"date\"])\n",
    "\n",
    "# ----------------------------\n",
    "# Helper functions for holiday/fasting\n",
    "# Fasting\n",
    "fasting = pd.read_csv(\n",
    "    \"/content/drive/MyDrive/ML Projects/Sales_Forecasting_Project/data/raw/fasting_periods.csv\"\n",
    ")\n",
    "fasting[\"start_date\"] = pd.to_datetime(fasting[\"start_date\"])\n",
    "fasting[\"end_date\"] = pd.to_datetime(fasting[\"end_date\"])\n",
    "\n",
    "# Holidays\n",
    "events = pd.read_csv(\n",
    "    \"/content/drive/MyDrive/ML Projects/Sales_Forecasting_Project/data/raw/ethiopian_events.csv\"\n",
    ")\n",
    "events[\"date\"] = pd.to_datetime(events[\"date\"])\n",
    "\n",
    "holiday_dates = events[events[\"is_holiday\"] == 1][\"date\"].sort_values()\n",
    "\n",
    "def is_fasting_day(d):\n",
    "    return int(((fasting[\"start_date\"] <= d) & (fasting[\"end_date\"] >= d)).any())\n",
    "\n",
    "def is_holiday_day(d):\n",
    "    return int(d in set(holiday_dates))\n",
    "\n",
    "def days_to_next_holiday(d):\n",
    "    future = holiday_dates[holiday_dates >= d]\n",
    "    return (future.iloc[0] - d).days if len(future) else np.nan\n",
    "\n",
    "def days_since_last_holiday(d):\n",
    "    past = holiday_dates[holiday_dates <= d]\n",
    "    return (d - past.iloc[-1]).days if len(past) else np.nan\n",
    "\n",
    "# ----------------------------\n",
    "# Feature Engineering for `daily` (consistent with Mv_jgUlwXkQp)\n",
    "# ----------------------------\n",
    "# CATEGORY TARGET ENCODING\n",
    "category_mean = (\n",
    "    train_df_for_features # Use the re-loaded train_df for feature calculation\n",
    "    .groupby(\"category\")[TARGET]\n",
    "    .mean()\n",
    ")\n",
    "daily[\"category_te\"] = daily[\"category\"].map(category_mean)\n",
    "\n",
    "global_mean = train_df_for_features[TARGET].mean()\n",
    "daily[\"category_te\"] = daily[\"category_te\"].fillna(global_mean)\n",
    "\n",
    "# SKU BEHAVIOR FEATURES (ROLLING)\n",
    "def add_sku_behavior_for_daily(df):\n",
    "    df = df.copy()\n",
    "    df[\"sku_avg_28d\"] = (\n",
    "        df.groupby(\"sku_id\")[\"daily_qty\"]\n",
    "        .rolling(28)\n",
    "        .mean()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    df[\"sku_std_28d\"] = (\n",
    "        df.groupby(\"sku_id\")[\"daily_qty\"]\n",
    "        .rolling(28)\n",
    "        .std()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "daily = add_sku_behavior_for_daily(daily)\n",
    "\n",
    "# ----------------------------\n",
    "# FINAL FEATURES LIST (Updated to include new features)\n",
    "# ----------------------------\n",
    "FEATURES_BASE = [\n",
    "    \"day_of_week\",\n",
    "    \"is_weekend\",\n",
    "    \"day_of_year\",\n",
    "    \"qty_lag_1\",\n",
    "    \"qty_lag_7\",\n",
    "    \"qty_lag_14\",\n",
    "    \"qty_roll_7\",\n",
    "    \"qty_roll_14\",\n",
    "    \"is_holiday\",\n",
    "    \"days_to_holiday\",\n",
    "    \"days_since_holiday\",\n",
    "    \"is_fasting\"\n",
    "]\n",
    "\n",
    "FEATURES = FEATURES_BASE + [\n",
    "    \"sku_avg_28d\",\n",
    "    \"sku_std_28d\",\n",
    "    \"category_te\",\n",
    "]\n",
    "\n",
    "# ----------------------------\n",
    "# DROP NA (Only after features ready) - Essential for features involving rolling means/stds\n",
    "# ----------------------------\n",
    "daily = daily.dropna(subset=FEATURES + [TARGET])\n",
    "\n",
    "# Start prediction\n",
    "FORECAST_DAYS = 90  # ~3 months\n",
    "\n",
    "all_daily_preds = []\n",
    "\n",
    "for sku in tqdm(daily[\"sku_id\"].unique(), desc=\"Rolling forward\"):\n",
    "    sku_hist = daily[daily[\"sku_id\"] == sku].copy()\n",
    "    sku_hist = sku_hist.sort_values(\"date\")\n",
    "\n",
    "    if len(sku_hist) < 30: # Keeping original 30 from the cell\n",
    "        continue\n",
    "\n",
    "    history_qty = sku_hist[TARGET].tolist()\n",
    "    last_date = sku_hist[\"date\"].max()\n",
    "    sku_category_te = sku_hist[\"category_te\"].iloc[-1] # category_te is constant for a SKU\n",
    "\n",
    "    for step in range(FORECAST_DAYS):\n",
    "        next_date = last_date + pd.Timedelta(days=1)\n",
    "\n",
    "        row = {\n",
    "            \"day_of_week\": next_date.weekday(),\n",
    "            \"is_weekend\": int(next_date.weekday() >= 5),\n",
    "            \"day_of_year\": next_date.dayofyear,\n",
    "            \"is_holiday\": is_holiday_day(next_date),\n",
    "            \"days_to_holiday\": days_to_next_holiday(next_date),\n",
    "            \"days_since_holiday\": days_since_last_holiday(next_date),\n",
    "            \"is_fasting\": is_fasting_day(next_date),\n",
    "            \"category_te\": sku_category_te # Add category_te to the row\n",
    "        }\n",
    "\n",
    "        # Lag & rolling features - based on history_qty (actuals + predictions)\n",
    "        row[\"qty_lag_1\"] = history_qty[-1]\n",
    "        row[\"qty_lag_7\"] = history_qty[-7] if len(history_qty) >= 7 else history_qty[-1]\n",
    "        row[\"qty_lag_14\"] = history_qty[-14] if len(history_qty) >= 14 else history_qty[-1]\n",
    "\n",
    "        row[\"qty_roll_7\"] = np.mean(history_qty[-7:])\n",
    "        row[\"qty_roll_14\"] = np.mean(history_qty[-14:])\n",
    "\n",
    "        # Calculate sku_avg_28d and sku_std_28d dynamically for forecast steps\n",
    "        rolling_window_28 = 28\n",
    "        if len(history_qty) >= rolling_window_28:\n",
    "            row[\"sku_avg_28d\"] = np.mean(history_qty[-rolling_window_28:])\n",
    "            row[\"sku_std_28d\"] = np.std(history_qty[-rolling_window_28:])\n",
    "        elif len(history_qty) > 0: # If history is present but less than rolling_window_28\n",
    "            row[\"sku_avg_28d\"] = np.mean(history_qty)\n",
    "            row[\"sku_std_28d\"] = np.std(history_qty)\n",
    "        else: # Fallback if history is completely empty (should not be reached if len(sku_hist) < 30 check is robust)\n",
    "            row[\"sku_avg_28d\"] = 0.0\n",
    "            row[\"sku_std_28d\"] = 0.0\n",
    "\n",
    "        # If std is 0 (e.g., all values are same), set to small epsilon to avoid potential issues if std is used as a denominator\n",
    "        if row[\"sku_std_28d\"] == 0:\n",
    "            row[\"sku_std_28d\"] = 1e-6\n",
    "\n",
    "        X = pd.DataFrame([row])[FEATURES]\n",
    "\n",
    "        y_pred = max(0, round(model.predict(X)[0]))\n",
    "\n",
    "        all_daily_preds.append({\n",
    "            \"sku_id\": sku,\n",
    "            \"date\": next_date,\n",
    "            \"predicted_qty\": y_pred\n",
    "        })\n",
    "\n",
    "        history_qty.append(y_pred)\n",
    "        last_date = next_date\n",
    "\n",
    "# Aggregate to monthly\n",
    "daily_pred_df = pd.DataFrame(all_daily_preds)\n",
    "daily_pred_df[\"year_month\"] = daily_pred_df[\"date\"].dt.to_period(\"M\")\n",
    "\n",
    "monthly_pred_df = (\n",
    "    daily_pred_df\n",
    "    .groupby([\"sku_id\", \"year_month\"])[\"predicted_qty\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Saving outputs\n",
    "daily_pred_df.to_csv(\n",
    "    DATA_PATH + \"rolling_daily_forecast.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "monthly_pred_df.to_csv(\n",
    "    DATA_PATH + \"rolling_monthly_forecast.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"Rolling-forward forecasting completed.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
